// Plamo Model Integration for Mastra with Langfuse
// Provides streaming responses compatible with Mastra frontend using Langfuse prompts
import { spawn } from 'child_process';
import { EventEmitter } from 'events';
import { langfuse } from './langfuse.js';
export class PlamoMastraProvider {
    constructor() {
        this.modelName = 'plamo';
        this.isStreaming = false;
    }
    // Get Langfuse prompt for Plamo
    async getPlamoPrompt(promptName, context = {}) {
        try {
            console.log(`üìù Fetching Langfuse prompt: ${promptName}`);
            const promptText = await langfuse.getPromptText(promptName, "production");
            if (!promptText) {
                throw new Error(`Prompt '${promptName}' not found in Langfuse`);
            }
            // Format the prompt with context
            const formattedPrompt = promptText
                .replace(/\{customerId\}/g, context.customerId || '')
                .replace(/\{sessionId\}/g, context.sessionId || '')
                .replace(/\{message\}/g, context.message || '')
                .replace(/\{context\}/g, JSON.stringify(context, null, 2));
            console.log(`‚úÖ Langfuse prompt loaded: ${promptName}`);
            return formattedPrompt;
        }
        catch (error) {
            console.error(`‚ùå Error fetching Langfuse prompt '${promptName}':`, error);
            // Fallback to direct prompt if Langfuse fails
            return context.message || "„Åì„Çì„Å´„Å°„ÅØ„ÄÅ‰Ωï„Åã„ÅäÊâã‰ºù„ÅÑ„Åß„Åç„Çã„Åì„Å®„ÅØ„ÅÇ„Çä„Åæ„Åô„ÅãÔºü";
        }
    }
    // Generate streaming response compatible with Mastra using Langfuse
    async generateStream(messages, options = {}) {
        const stream = new EventEmitter();
        try {
            // Extract the last user message
            const lastMessage = messages[messages.length - 1];
            let userInput = lastMessage.content || lastMessage.text || "";
            
            // Handle Mastra format where content is an array of objects
            if (Array.isArray(userInput)) {
                userInput = userInput.map(item => item.text || item.content || "").join(" ");
            }
            
            console.log(`ü§ñ Plamo generating response for: ${userInput.substring(0, 50)}...`);
            // Determine which Langfuse prompt to use based on context
            let promptName = 'Domestic-customer-identification'; // Default
            // Check message content to determine appropriate prompt
            if (userInput.includes('‰øÆÁêÜ') || userInput.includes('ÊïÖÈöú') || userInput.includes('ÂïèÈ°å')) {
                promptName = 'Domestic-repair-agent';
            }
            else if (userInput.includes('‰∫àÁ¥Ñ') || userInput.includes('Ë®™Âïè') || userInput.includes('„Çπ„Ç±„Ç∏„É•„Éº„É´')) {
                promptName = 'Domestic-repair-scheduling';
            }
            else if (userInput.includes('Â±•Ê≠¥') || userInput.includes('ÈÅéÂéª') || userInput.includes('Ë®òÈå≤')) {
                promptName = 'Domestic-repair-history-ticket';
            }
            // Get context from options
            const context = {
                message: userInput,
                customerId: options.customerId,
                sessionId: options.sessionId,
                ...options.context
            };
            // Get formatted prompt from Langfuse
            const formattedPrompt = await this.getPlamoPrompt(promptName, context);
            const ollama = spawn('ollama', ['run', this.modelName, formattedPrompt]);
            let fullResponse = '';
            let isFirstChunk = true;
            ollama.stdout.on('data', (data) => {
                const chunk = data.toString().trim();
                if (chunk) {
                    fullResponse += chunk;
                    // Emit Mastra f0ed streaming format
                    stream.emit('data', { content: chunk });
                    if (isFirstChunk) {
                        isFirstChunk = false;
                    }
                }
            });
            ollama.stderr.on('data', (data) => {
                console.error('Plamo stderr:', data.toString());
            });
            ollama.on('close', (code) => {
                if (code === 0) {
                    // Emit final chunk
                    const finalChunk = {
                        type: 'text-delta',
                        textDelta: '',
                        finishReason: 'stop'
                    };
                    stream.emit('data', finalChunk);
                    stream.emit('end', {
                        text: fullResponse,
                        usage: {
                            promptTokens: formattedPrompt.length,
                            completionTokens: fullResponse.length,
                            totalTokens: formattedPrompt.length + fullResponse.length
                        }
                    });
                }
                else {
                    stream.emit('error', new Error(`Plamo process failed with code ${code}`));
                }
            });
            ollama.on('error', (err) => {
                stream.emit('error', err);
            });
        }
        catch (error) {
            stream.emit('error', error);
        }
        return stream;
    }
    // Generate non-streaming response using Langfuse
    async generate(messages, options = {}) {
        return new Promise(async (resolve, reject) => {
            try {
                // Extract the last user message
                const lastMessage = messages[messages.length - 1];
                let userInput = lastMessage.content || lastMessage.text || "";
                
                // Handle Mastra format where content is an array of objects
                if (Array.isArray(userInput)) {
                    userInput = userInput.map(item => item.text || item.content || "").join(" ");
                }
                // Determine which Langfuse prompt to use
                let promptName = 'Domestic-customer-identification'; // Default
                if (userInput.includes('‰øÆÁêÜ') || userInput.includes('ÊïÖÈöú') || userInput.includes('ÂïèÈ°å')) {
                    promptName = 'Domestic-repair-agent';
                }
                else if (userInput.includes('‰∫àÁ¥Ñ') || userInput.includes('Ë®™Âïè') || userInput.includes('„Çπ„Ç±„Ç∏„É•„Éº„É´')) {
                    promptName = 'Domestic-repair-scheduling';
                }
                else if (userInput.includes('Â±•Ê≠¥') || userInput.includes('ÈÅéÂéª') || userInput.includes('Ë®òÈå≤')) {
                    promptName = 'Domestic-repair-history-ticket';
                }
                // Get context from options
                const context = {
                    message: userInput,
                    customerId: options.customerId,
                    sessionId: options.sessionId,
                    ...options.context
                };
                // Get formatted prompt from Langfuse
                const formattedPrompt = await this.getPlamoPrompt(promptName, context);
                const ollama = spawn('ollama', ['run', this.modelName, formattedPrompt]);
                let output = '';
                let error = '';
                ollama.stdout.on('data', (data) => {
                    output += data.toString();
                });
                ollama.stderr.on('data', (data) => {
                    error += data.toString();
                });
                ollama.on('close', (code) => {
                    if (code === 0) {
                        resolve({
                            text: output.trim(),
                            usage: {
                                promptTokens: formattedPrompt.length,
                                completionTokens: output.length,
                                totalTokens: formattedPrompt.length + output.length
                            }
                        });
                    }
                    else {
                        reject(new Error(error || 'Plamo process failed'));
                    }
                });
                ollama.on('error', (err) => {
                    reject(err);
                });
            }
            catch (error) {
                reject(error);
            }
        });
    }
    // Health check
    async healthCheck() {
        try {
            const testMessages = [{ role: 'user', content: '„Åì„Çì„Å´„Å°„ÅØ' }];
            const response = await this.generate(testMessages);
            return {
                status: 'healthy',
                model: this.modelName,
                response: response.text.substring(0, 50) + '...',
                langfuse: 'connected'
            };
        }
        catch (error) {
            return {
                status: 'unhealthy',
                model: this.modelName,
                error: error instanceof Error ? error.message : 'Unknown error',
                langfuse: 'error'
            };
        }
    }
}
// Export singleton instance
export const plamoProvider = new PlamoMastraProvider();
