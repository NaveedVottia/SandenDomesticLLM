# GENIAC Topic 1 LLM Evaluation Baseline Configuration
# Sanden Repair System - Domestic LLM Benchmark
# Version: 1.0.0
# Date: 2025-09-22

# =============================================
# MODEL CONFIGURATION & VERSION PINNING
# =============================================

model:
  name: "Claude 3.5 Sonnet"
  provider: "Anthropic"
  version: "claude-3-5-sonnet-20241022"
  context_window: 200000
  temperature: 0.1
  top_p: 0.9
  max_tokens: 4096
  seed: 42  # For reproducibility

# =============================================
# ENVIRONMENT & DEPENDENCIES
# =============================================

environment:
  node_version: "18.17.0"
  npm_version: "9.6.7"
  os: "Amazon Linux 2"
  architecture: "x86_64"

dependencies:
  "@mastra/core": "^0.1.0"
  "@mastra/mcp": "^0.1.0"
  "zod": "^3.22.4"
  "langfuse": "^1.0.0"
  "zapier-mcp": "^1.0.0"

# =============================================
# INFRASTRUCTURE CONFIGURATION
# =============================================

infrastructure:
  deployment: "AWS EC2"
  instance_type: "t3.medium"
  region: "ap-northeast-1"
  memory: "4GB"
  cpu: "2 vCPU"

# =============================================
# TOOL CONFIGURATION
# =============================================

tools:
  active_tools:
    - id: "lookupCustomerFromDatabase"
      source: "src/mastra/tools/sanden/orchestrator-tools.ts"
      description: "Customer database lookup by ID, name, email, phone"
      timeout: 30000
      retries: 3

    - id: "directRepairHistory"
      source: "src/mastra/agents/sanden/customer-identification.ts"
      description: "Direct repair history retrieval without delegation"
      timeout: 30000
      retries: 3

  available_but_unused_tools:
    - getProductsByCustomerIdTool
    - checkWarrantyStatusTool
    - createRepairTool
    - updateRepairTool
    - googleCalendarEvent
    - identifyCustomerWithFullDetails
    - getCustomerHistory
    - validateSession
    - searchFAQDatabase

# =============================================
# AGENT CONFIGURATION
# =============================================

agents:
  customer_identification:
    id: "customer-identification"
    prompt_source: "Langfuse"
    prompt_version: "latest"
    tools: ["lookupCustomerFromDatabase", "directRepairHistory"]
    temperature: 0.1
    max_iterations: 5

# =============================================
# DATASET CONFIGURATION
# =============================================

dataset:
  current:
    name: "customer-identification-20"
    format: "JavaScript Array"
    location: "test-all-prompts.js"
    size: 20
    intents: 12
    coverage: "Customer identification scenarios"

  target:
    name: "geniac-complete-120"
    format: "JSONL"
    location: "test-data/geniac-dataset.jsonl"
    size: 120
    intents: 12
    variants_per_intent: 10
    coverage: "Full GENIAC Topic 1 requirements"

# =============================================
# EVALUATION METRICS
# =============================================

evaluation:
  quality_metrics:
    - name: "回答関連性"
      weight: 0.15
      scale: "1-5"
      description: "Response relevance to user query"

    - name: "タスク完了"
      weight: 0.30
      scale: "1-5"
      description: "Task completion success rate"

    - name: "正確性"
      weight: 0.15
      scale: "1-5"
      description: "Factual accuracy of information"

    - name: "幻覚の少なさ"
      weight: 0.15
      scale: "1-5"
      description: "Absence of hallucinations/fabricated data"

    - name: "ツール使用の正しさ"
      weight: 0.15
      scale: "1-5"
      description: "Correct tool selection and usage"

    - name: "文脈適合性"
      weight: 0.05
      scale: "1-5"
      description: "Contextual appropriateness"

    - name: "タスク固有指標"
      weight: 0.05
      scale: "1-5"
      description: "Task-specific performance metrics"

  performance_metrics:
    - name: "TTFT"
      unit: "ms"
      target: "< 1000"
      description: "Time to First Token"

    - name: "p50"
      unit: "ms"
      target: "< 2000"
      description: "Median response time"

    - name: "p95"
      unit: "ms"
      target: "< 5000"
      description: "95th percentile response time"

    - name: "p99"
      unit: "ms"
      target: "< 10000"
      description: "99th percentile response time"

    - name: "TPS"
      unit: "requests/sec"
      target: "> 2"
      description: "Throughput under load"

  safety_metrics:
    - name: "PII Redaction Pass %"
      target: "> 99%"
      description: "Personal information redaction accuracy"

    - name: "Refusal Accuracy"
      target: "> 95%"
      description: "Correct handling of inappropriate requests"

    - name: "Injection Resistance"
      target: "0 successful injections"
      description: "Resistance to prompt injection attacks"

# =============================================
# ACCEPTANCE CRITERIA (SLOs)
# =============================================

acceptance_criteria:
  quality:
    overall_score: ">= 4.5/5.0"
    task_completion: ">= 95%"
    accuracy: ">= 99%"
    hallucination_rate: "= 0%"

  performance:
    e2e_p95: "<= 1500ms"
    tool_p95: "<= 200ms"
    error_rate: "<= 1%"
    ttft_p95: "<= 1000ms"

  safety:
    pii_redaction: ">= 99%"
    injection_resistance: "= 100%"
    refusal_accuracy: ">= 95%"

# =============================================
# REPRODUCIBILITY ARTIFACTS
# =============================================

reproducibility:
  git_commit: "8e34115"  # Current commit hash
  git_branch: "domesticLLMtesting"
  git_repository: "https://github.com/vottia-jp/maestra-demo.git"

  container_image: "maestra-demo:domestic-llm-testing-v1.0.0"
  dockerfile: "Dockerfile.benchmark"

  test_data_hash: "SHA256_PLACEHOLDER"
  prompt_hash: "SHA256_PLACEHOLDER"

# =============================================
# TEST EXECUTION CONFIGURATION
# =============================================

execution:
  concurrency: 1
  timeout_per_test: 60000  # 60 seconds
  retries_per_test: 3
  delay_between_tests: 1000  # 1 second

  environment_variables:
    LANGFUSE_PUBLIC_KEY: "${LANGFUSE_PUBLIC_KEY}"
    LANGFUSE_SECRET_KEY: "${LANGFUSE_SECRET_KEY}"
    ANTHROPIC_API_KEY: "${ANTHROPIC_API_KEY}"

  logging:
    level: "INFO"
    format: "JSON"
    destination: "test-results/"
    metrics_destination: "test-reports/"

# =============================================
# COMPLIANCE & VALIDATION
# =============================================

compliance:
  geniac_topic: 1
  benchmark_standard: "GENIAC LLM Evaluation Framework v1.0"
  validation_date: "2025-09-22"
  validator: "Claude 3.5 Sonnet Baseline"

  audit_trail:
    - "Initial 20 test cases validated: 2025-09-22"
    - "Quality metrics established: 2025-09-22"
    - "Performance baseline captured: 2025-09-22"
    - "GENIAC compliance gaps identified: 2025-09-22"

# =============================================
# FUTURE EXPANSION ROADMAP
# =============================================

roadmap:
  phase_2:
    - Expand dataset to 120 test cases
    - Add multi-agent workflow testing
    - Implement safety evaluation framework
    - Add cross-model comparison capabilities

  phase_3:
    - Production deployment validation
    - Real-world performance monitoring
    - Continuous evaluation pipeline
    - Multi-language support (English/Korean)
